# Copyright Â© 2025 Sierra Labs LLC
# SPDX-License-Identifier: AGPL-3.0-only
# License-Filename: LICENSE

"""
Fast path consumer for Redis Streams.

Reads events from Redis Streams, batches them, writes to SQLite,
and publishes CDC events for slow path workers.
"""

import json
import asyncio
import logging
from typing import Dict, List, Any, Optional
import redis

from .batch_manager import BatchManager
from .cdc_publisher import CDCPublisher
from ..database.writer import SQLiteBatchWriter

logger = logging.getLogger(__name__)


class FastPathConsumer:
    """
    High-throughput consumer that writes raw events with zero blocking.
    
    Target: <10ms per batch at P95.
    
    Features:
    - Redis Streams XREADGROUP for consumer groups
    - Batch accumulation (100 events or 100ms timeout)
    - SQLite batch writes with compression
    - CDC event publishing
    - Dead Letter Queue (DLQ) for failed messages
    - Pending Entries List (PEL) retry handling
    """

    def __init__(
        self,
        redis_client: redis.Redis,
        sqlite_writer: SQLiteBatchWriter,
        cdc_publisher: CDCPublisher,
        stream_name: str = "telemetry:events",
        consumer_group: str = "processors",
        consumer_name: str = "fast-path-1",
        batch_size: int = 100,
        batch_timeout: float = 0.1,
        block_ms: int = 1000,
        max_retries: int = 3,
    ):
        """
        Initialize fast path consumer.

        Args:
            redis_client: Redis client instance
            sqlite_writer: SQLite batch writer
            cdc_publisher: CDC publisher
            stream_name: Redis Stream name
            consumer_group: Consumer group name
            consumer_name: Consumer name (unique per instance)
            batch_size: Maximum batch size
            batch_timeout: Batch timeout in seconds
            block_ms: Blocking timeout for XREADGROUP (ms)
            max_retries: Maximum retries before DLQ
        """
        self.redis_client = redis_client
        self.sqlite_writer = sqlite_writer
        self.cdc_publisher = cdc_publisher
        self.stream_name = stream_name
        self.consumer_group = consumer_group
        self.consumer_name = consumer_name
        self.batch_manager = BatchManager(batch_size, batch_timeout)
        self.block_ms = block_ms
        self.max_retries = max_retries
        self.running = False
        self.dlq_stream = "telemetry:dlq"

    async def _ensure_consumer_group(self) -> None:
        """Ensure consumer group exists, create if not."""
        try:
            self.redis_client.xgroup_create(
                self.stream_name,
                self.consumer_group,
                id="0",
                mkstream=True
            )
            logger.info(f"Created consumer group {self.consumer_group}")
        except redis.ResponseError as e:
            if "BUSYGROUP" in str(e):
                # Group already exists, that's fine
                logger.debug(f"Consumer group {self.consumer_group} already exists")
            else:
                raise

    async def _read_messages(self) -> List[Dict[str, Any]]:
        """
        Read messages from Redis Streams using XREADGROUP.

        Returns:
            List of message dictionaries with 'id' and 'event' keys
        """
        try:
            # Read from stream with consumer group
            # Note: redis-py xreadgroup returns list of tuples: [(stream_name, [(id, {field: value}), ...])]
            messages = self.redis_client.xreadgroup(
                self.consumer_group,
                self.consumer_name,
                {self.stream_name: ">"},
                count=self.batch_manager.batch_size,
                block=self.block_ms
            )

            if not messages:
                return []

            # Parse messages
            # queue_writer stores events as flat key-value pairs:
            # event_id, enqueued_at, platform, hook_type, timestamp, payload (JSON), metadata (JSON), etc.
            result = []
            for stream_name, stream_messages in messages:
                for message_id, fields in stream_messages:
                    # Convert message_id to string
                    msg_id = message_id.decode('utf-8') if isinstance(message_id, bytes) else str(message_id)
                    
                    try:
                        # Reconstruct event from flat fields
                        event = {}
                        
                        # Helper to decode field value
                        def decode_field(key, value):
                            if isinstance(value, bytes):
                                return value.decode('utf-8')
                            return str(value)
                        
                        # Copy top-level fields
                        for key, value in fields.items():
                            key_str = key.decode('utf-8') if isinstance(key, bytes) else str(key)
                            val_str = decode_field(key_str, value)
                            
                            # Parse JSON fields (payload, metadata)
                            if key_str in ('payload', 'metadata'):
                                try:
                                    event[key_str] = json.loads(val_str)
                                except json.JSONDecodeError:
                                    event[key_str] = {}
                            else:
                                # Store other fields as-is
                                event[key_str] = val_str
                        
                        # Ensure required fields exist
                        if 'event_id' not in event:
                            event['event_id'] = msg_id
                        if 'session_id' not in event:
                            # Use external_session_id if available
                            event['session_id'] = event.get('external_session_id', '')
                        
                        result.append({
                            'id': msg_id,
                            'event': event
                        })
                        
                    except Exception as e:
                        logger.error(f"Failed to parse event from message {msg_id}: {e}")
                        result.append({
                            'id': msg_id,
                            'event': None,
                            'error': str(e)
                        })

            return result

        except redis.ConnectionError as e:
            logger.error(f"Redis connection error: {e}")
            return []
        except Exception as e:
            logger.error(f"Error reading messages: {e}")
            return []

    async def _process_batch(self, messages: List[Dict[str, Any]]) -> List[str]:
        """
        Process batch of messages: write to SQLite and publish CDC events.

        Args:
            messages: List of message dictionaries

        Returns:
            List of message IDs that were successfully processed
        """
        if not messages:
            return []

        # Extract events (skip None events from parse errors)
        events = []
        valid_message_ids = []
        for msg in messages:
            if msg['event'] is not None:
                events.append(msg['event'])
                valid_message_ids.append(msg['id'])
            else:
                # Invalid event - send to DLQ immediately
                await self._handle_failed_message(msg['id'], msg.get('event'), retry_count=self.max_retries)

        if not events:
            return []

        try:
            # Write to SQLite
            sequences = await self.sqlite_writer.write_batch(events)

            # Publish CDC events (fire-and-forget, synchronous call)
            for sequence, event in zip(sequences, events):
                self.cdc_publisher.publish(sequence, event)

            logger.debug(f"Processed batch: {len(events)} events, sequences {sequences[0]}-{sequences[-1]}")
            return valid_message_ids

        except Exception as e:
            logger.error(f"Failed to process batch: {e}")
            # Don't ACK messages - they'll retry via PEL
            return []

    async def _ack_messages(self, message_ids: List[str]) -> None:
        """
        Acknowledge processed messages.

        Args:
            message_ids: List of message IDs to acknowledge
        """
        if not message_ids:
            return

        try:
            self.redis_client.xack(
                self.stream_name,
                self.consumer_group,
                *message_ids
            )
        except Exception as e:
            logger.error(f"Failed to ACK messages: {e}")

    async def _handle_failed_message(self, message_id: str, event: Optional[Dict], retry_count: int) -> None:
        """
        Handle failed message: send to DLQ if max retries exceeded.

        Args:
            message_id: Message ID
            event: Event data (may be None)
            retry_count: Current retry count
        """
        if retry_count >= self.max_retries:
            # Send to Dead Letter Queue
            try:
                dlq_data = {
                    'message_id': message_id,
                    'retry_count': retry_count,
                    'event': json.dumps(event) if event else 'null',
                }
                self.redis_client.xadd(
                    self.dlq_stream,
                    dlq_data,
                    maxlen=1000,
                    approximate=True
                )
                logger.warning(f"Sent message {message_id} to DLQ after {retry_count} retries")
            except Exception as e:
                logger.error(f"Failed to send message to DLQ: {e}")

    async def _process_pending_messages(self) -> None:
        """Process pending messages from PEL (retry failed messages)."""
        try:
            # Get pending messages for this consumer
            pending = self.redis_client.xpending_range(
                self.stream_name,
                self.consumer_group,
                min="-",
                max="+",
                count=100,
                consumername=self.consumer_name
            )

            if not pending:
                return

            # Claim messages that are older than retry timeout
            message_ids = [msg['message_id'] for msg in pending]
            if message_ids:
                claimed = self.redis_client.xclaim(
                    self.stream_name,
                    self.consumer_group,
                    self.consumer_name,
                    min_idle_time=300000,  # 5 minutes
                    message_ids=message_ids
                )

                # Process claimed messages
                if claimed:
                    messages = []
                    for msg_id, fields in claimed:
                        event_data = fields.get(b'data', b'{}')
                        try:
                            event = json.loads(event_data.decode('utf-8'))
                            messages.append({
                                'id': msg_id.decode('utf-8') if isinstance(msg_id, bytes) else msg_id,
                                'event': event
                            })
                        except Exception as e:
                            logger.error(f"Failed to parse claimed message: {e}")

                    if messages:
                        processed_ids = await self._process_batch(messages)
                        await self._ack_messages(processed_ids)

        except Exception as e:
            logger.error(f"Error processing pending messages: {e}")

    async def run(self) -> None:
        """
        Main consumer loop.

        Continuously reads from Redis Streams, batches events,
        writes to SQLite, and publishes CDC events.
        """
        self.running = True
        await self._ensure_consumer_group()

        logger.info(f"Fast path consumer started: {self.consumer_name}")

        while self.running:
            try:
                # Process pending messages first (retries)
                await self._process_pending_messages()

                # Read new messages
                messages = await self._read_messages()

                if messages:
                    # Track message IDs with events
                    message_map = {msg['id']: msg['event'] for msg in messages if msg['event']}
                    
                    # Add events to batch
                    for msg in messages:
                        if msg['event']:
                            ready = self.batch_manager.add_event(msg['event'])
                            if ready:
                                # Batch is full, flush it
                                batch = self.batch_manager.get_batch()
                                # Reconstruct messages for batch processing
                                batch_messages = []
                                for event in batch:
                                    # Find matching message ID (use first available)
                                    for msg_id, event_data in message_map.items():
                                        if event_data == event:
                                            batch_messages.append({'id': msg_id, 'event': event})
                                            break
                                
                                processed_ids = await self._process_batch(batch_messages)
                                await self._ack_messages(processed_ids)
                                
                                # Remove processed messages from map
                                for msg_id in processed_ids:
                                    message_map.pop(msg_id, None)

                # Check if timeout-based flush is needed
                if self.batch_manager.should_flush() and not self.batch_manager.is_empty():
                    batch = self.batch_manager.get_batch()
                    # For timeout flush, we need to process remaining messages
                    # But we don't have message IDs - need to read pending or skip ACK
                    # For now, write events but note that ACK will happen on next read
                    if batch:
                        # Write events (they'll be ACKed when we process their messages)
                        sequences = await self.sqlite_writer.write_batch(batch)
                        # Publish CDC events
                        for sequence, event in zip(sequences, batch):
                            self.cdc_publisher.publish(sequence, event)
                        logger.debug(f"Timeout flush: wrote {len(batch)} events")

                # Small sleep to prevent tight loop
                await asyncio.sleep(0.01)

            except asyncio.CancelledError:
                logger.info("Consumer cancelled")
                break
            except Exception as e:
                logger.error(f"Error in consumer loop: {e}")
                await asyncio.sleep(1)  # Back off on error

        logger.info("Fast path consumer stopped")

    def stop(self) -> None:
        """Stop the consumer."""
        self.running = False

