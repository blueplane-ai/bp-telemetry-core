# Meeting Prep: Ryan 1:1 - Analytics & Skills Integration
**Date**: December 3, 2025  
**Prepared**: Based on sync meeting notes + codebase analysis

---

## Executive Summary

This document prepares for the 1:1 with Ryan to align analytics implementation with his skills/journaling work. It's grounded in **actual codebase state** and identifies concrete gaps and opportunities.

**Key Insight**: We have solid foundations (SQLite traces, sessions, conversations) but need to design the **API contract** between analytics and skills, especially for PR-level insights which require new capabilities.

---

## Current Implementation State

### ✅ What We Have (Available Now)

#### Data Layer (SQLite)
- **`cursor_raw_traces`**: 51,925+ traces with comprehensive fields
  - Model metadata: `model_name`, `model_cost_cents`, `model_response_count` (extracted from `usageData`)
  - Session tracking: `external_session_id`, `workspace_hash`
  - Event types: bubbles, composer sessions, generations, prompts
  - Timing: `timestamp`, `created_at`, `completed_at`
  - Code metrics: `lines_added`, `lines_removed`
- **`claude_raw_traces`**: Schema ready, 0 traces currently
  - Model info: `message_model` field available
  - Token usage: `input_tokens`, `output_tokens`, cache metrics
- **`cursor_sessions`**: 196 sessions tracked
  - Session lifecycle: `started_at`, `ended_at`
  - Workspace mapping: `workspace_hash`, `workspace_path`
- **`conversations`**: 142 reconstructed conversations
  - Turn-level data with timestamps
  - Links to sessions via `session_id`

#### Processing Layer
- Analytics service exists (`src/analytics/`) but is early stage
- DuckDB processing pipeline (reads from SQLite)
- Incremental processing with state tracking
- **Current gap**: Only ~1% of traces processed (540/51,925)

#### Model Metadata Status
- **Cursor**: ✅ Model extraction in progress (`feature/cursor-model-extraction` branch)
  - `model_name` field populated from `usageData` dictionary
  - Can track which model was used per generation
- **Claude Code**: ✅ Model info available in `message_model` field
- **Status**: Model awareness is **partially implemented** and can be enhanced

### ❌ What We're Missing (Critical Gaps)

#### PR Tracking & Boundaries
- **No PR detection**: Can't determine when a PR started
- **No PR-to-traces mapping**: Can't link traces to specific PRs
- **No git integration**: No commit/PR metadata capture
- **Impact**: PR-level insight skill **cannot work** without this

#### API Layer for Skills
- **No REST API**: Skills can't query analytics layer
- **No query functions**: No `get_session_summary()`, `get_daily_summary()`, `get_pr_traces()`
- **No data contracts**: No JSON schemas for SessionSummary, PRSummary, TraceBundle
- **Impact**: Skills need to query SQLite directly (fragile, not scalable)

#### Day/Session Grouping Logic
- **Sessions exist** but no "day" grouping exposed
- **No time-window queries**: Can't easily get "all traces for Dec 3"
- **No session-to-day mapping**: Can't aggregate sessions into days
- **Impact**: Long-form report skill needs day-level aggregation

#### Journaling Integration
- **No journaling data capture**: Journaling answers not stored
- **No journaling-to-traces linking**: Can't correlate journaling with trace data
- **Impact**: Skills can't combine journaling + traces for insights

---

## Ryan's Vision (From Meeting Notes)

### Core Vision Elements

1. **Skills-First Analytics Engine**
   - Long-form report skill (session/day/period)
   - PR-level quick insight skill
   - Skills pull right traces at right time-window

2. **Journaling as High-Signal Input**
   - Guided journaling (numbered questions, voice-friendly)
   - Configurable verbosity (minimal vs. verbose)
   - Captures missing context that traces can't see

3. **Model-Aware Insights**
   - Know which model was used per call
   - Model-specific insights and error attribution
   - **Status**: Partially available (Cursor has model_name, Claude has message_model)

4. **Single-Player Introspection**
   - "Me and my LLMs" framing
   - Personal productivity insights
   - Foundation for future multiplayer

5. **Learning-Oriented Analytics**
   - Assess current level
   - Track progression over time
   - Adapt to different personas (AI champion, junior, novice)

---

## Refined Meeting Agenda

### 1. **Align on "Introspection, Single-Player" Vision** (15 min)
**Goal**: Confirm what ideal day/PR introspection looks like for Ryan personally

**Discussion Points**:
- What would a perfect "daily introspection report" show?
- What would a perfect "PR insight" show?
- What questions should these answer?
- How should they feel? (coaching vs. reporting)

**Current State Check**:
- ✅ We have session data (196 sessions)
- ✅ We have trace data (51K+ traces)
- ❌ We don't have day-level grouping
- ❌ We don't have PR tracking

**Key Questions for Ryan**:
- "If Blueplane nailed introspection for you personally, what would it show?"
- "What's the most valuable insight you'd want from a PR?"
- "How do you want to consume these insights?" (CLI, dashboard, attached to PR?)

---

### 2. **Define Data Contract Between Analytics and Skills** (20 min)
**Goal**: Design the API contract that skills will consume

**Critical Discussion**:

#### A. Long-Form Report Skill Contract

**What Ryan needs**:
- Session/day-level insights
- Time-window: "today", "yesterday", "this week"
- Rich context: traces, journaling, model usage

**What we can provide NOW**:
```python
# Potential API shape (needs implementation)
def get_daily_summary(date: str) -> DailySummary:
    """
    Returns:
    - sessions: List[SessionSummary]
    - total_interactions: int
    - total_lines_changed: int
    - platform_breakdown: Dict[str, int]
    - model_usage: Dict[str, int]  # model_name -> count
    - peak_hours: List[int]
    - workspace_activity: Dict[str, int]
    """
```

**What we CAN'T provide yet**:
- Journaling data (not captured)
- PR boundaries (no PR tracking)
- Day-level grouping (needs implementation)

**Key Questions**:
- "If you could call one API for the long-form report skill, what would you want it to return?"
- "What's the minimal data structure that would be useful?"
- "Should this be a single API call or multiple calls?"

#### B. PR-Level Insight Skill Contract

**What Ryan needs**:
- PR start time detection
- All traces relevant to a PR
- Quick insights (shorter than long-form report)

**What we CAN'T provide yet**:
- ❌ PR detection (no git integration)
- ❌ PR-to-traces mapping
- ❌ PR start time

**Critical Gap**: This skill **cannot work** without PR tracking. Need to discuss:
- How to detect PR start? (git hooks? commit timestamps? manual tagging?)
- How to map traces to PRs? (workspace + time window? git branch tracking?)
- What's the MVP approach? (manual PR ID input? automatic detection?)

**Key Questions**:
- "How do you envision triggering PR insights?" (on PR open? on demand?)
- "What's the minimal PR tracking we need to make this work?"
- "Can we start with manual PR ID input, then automate later?"

#### C. Data Structure Design

**Proposed JSON Schemas** (to discuss):

```json
{
  "SessionSummary": {
    "session_id": "string",
    "platform": "cursor|claude",
    "started_at": "ISO8601",
    "ended_at": "ISO8601",
    "duration_minutes": "number",
    "interaction_count": "number",
    "lines_added": "number",
    "lines_removed": "number",
    "models_used": ["string"],
    "workspace_hash": "string"
  },
  "DailySummary": {
    "date": "YYYY-MM-DD",
    "sessions": ["SessionSummary"],
    "total_interactions": "number",
    "total_lines_changed": "number",
    "platform_breakdown": {"cursor": 32, "claude": 15},
    "model_usage": {"claude-4.5-opus": 10, "claude-sonnet": 5},
    "peak_hours": [14, 15, 16],
    "workspace_activity": {"hash1": 23, "hash2": 12}
  },
  "PRSummary": {
    "pr_id": "string",  // GitHub PR number or branch name
    "start_time": "ISO8601",  // When PR work likely started
    "end_time": "ISO8601",  // PR creation/merge time
    "traces": ["TraceReference"],  // Relevant trace IDs
    "sessions": ["SessionSummary"],  // Sessions during PR work
    "total_interactions": "number",
    "code_changes": {"added": 100, "removed": 50},
    "models_used": ["string"]
  },
  "TraceBundle": {
    "time_window": {"start": "ISO8601", "end": "ISO8601"},
    "traces": ["TraceData"],  // Full trace data with decompressed event_data
    "summary": "DailySummary|PRSummary|SessionSummary"
  }
}
```

**Key Questions**:
- "Do these structures make sense for your skills?"
- "What's missing? What's unnecessary?"
- "How should we handle large trace bundles?" (pagination? compression?)

---

### 3. **Map Journaling Questions to Analytics Needs** (15 min)
**Goal**: Identify which journaling questions are essential for high-value insights

**Ryan's Journaling Updates**:
- Numbered questions for easy voice response
- Examples for each question
- Potentially too many questions (needs refinement)

**Discussion Points**:
- Which journaling questions unlock the most valuable insights?
- What's the minimal set vs. full configuration?
- How should journaling data be stored and linked to traces?

**Current State**:
- ❌ No journaling data capture yet
- ❌ No journaling-to-traces linking

**Key Questions**:
- "Which journaling questions would make the biggest difference for insights?"
- "What's the minimal journaling set that still provides value?"
- "How should we store journaling answers?" (separate table? linked to sessions?)
- "Should journaling be optional or required for certain insights?"

**Proposed Approach**:
- Store journaling in `journaling_entries` table
- Link to sessions via `session_id`
- Link to days via `date`
- Link to PRs via `pr_id` (when PR tracking exists)
- Make journaling optional but enhance insights when present

---

### 4. **Discuss Model-Aware Traces & Future-Proofing** (10 min)
**Goal**: Align on model metadata usage and ensure analytics can leverage it

**Current State**:
- ✅ Cursor: `model_name` field exists (extraction in progress)
- ✅ Claude Code: `message_model` field exists
- ⚠️ Not all traces have model info yet (extraction ongoing)

**Discussion Points**:
- How will Ryan use model metadata in insights?
- What model-specific insights are most valuable?
- How should we structure model info for future expansion?

**Key Questions**:
- "What would you do differently if you knew the model per call?"
- "What model-specific insights would be most valuable?" (error attribution? cost analysis? performance comparison?)
- "How should we handle traces without model info?" (infer? skip? flag?)

**Proposed Schema Enhancement**:
```python
# Current: model_name (string)
# Proposed: model_info (JSON)
{
  "name": "claude-4.5-opus",
  "family": "claude-opus",
  "provider": "anthropic",
  "version": "4.5",
  "capabilities": ["thinking", "code", "reasoning"]
}
```

**Future-Proofing**:
- Design analytics queries to handle missing model info gracefully
- Structure for model family comparisons (claude-sonnet vs. claude-opus)
- Enable cost analysis when model + token data available

---

### 5. **Agree on Next 1-2 Concrete Experiences to Ship** (15 min)
**Goal**: Define concrete deliverables for next 1-2 weeks

**Proposed Options**:

#### Option A: Daily Introspection Report (Easier)
**What**:
- CLI command: `blueplane report today`
- Returns: DailySummary JSON + formatted output
- Includes: sessions, interactions, code changes, model usage, peak hours

**What Aaron needs to build**:
- Day-level grouping logic
- `get_daily_summary()` query function
- CLI command implementation
- Basic formatting

**What Ryan needs to build**:
- Skill that consumes DailySummary
- Prompt engineering for insights
- Output formatting

**Timeline**: 1 week

#### Option B: PR Insight (Harder, requires PR tracking)
**What**:
- CLI command: `blueplane pr-insight <pr-id>`
- Returns: PRSummary JSON + quick insights
- Includes: PR traces, sessions, code changes, model usage

**What Aaron needs to build**:
- PR tracking (git integration or manual input)
- PR-to-traces mapping
- `get_pr_summary()` query function
- CLI command implementation

**What Ryan needs to build**:
- PR insight skill
- Prompt engineering for PR-specific insights

**Timeline**: 2-3 weeks (depends on PR tracking approach)

#### Option C: Session Summary API (Foundation)
**What**:
- REST API endpoint: `GET /api/v1/sessions/{session_id}/summary`
- Returns: SessionSummary JSON
- Foundation for both daily and PR insights

**What Aaron needs to build**:
- REST API layer (FastAPI or similar)
- `get_session_summary()` query function
- API endpoint implementation

**What Ryan needs to build**:
- Skills that consume API
- Test integration

**Timeline**: 1 week

**Recommendation**: Start with **Option C (Session Summary API)** as foundation, then build **Option A (Daily Report)** on top of it. This gives Ryan something to work with immediately while we design PR tracking.

**Key Questions**:
- "Which experience would be most valuable to you right now?"
- "What's the fastest path to a working demo?"
- "Can we start with manual PR ID input for PR insights?"

---

## Additional Topics to Cover (If Time Permits)

### A. Session Definition Clarification
**Current State**:
- Cursor: Session = IDE window instance (can have multiple conversations)
- Claude Code: Session = Conversation (1:1 mapping)

**Questions**:
- For analytics purposes, what defines a "session"?
- Should we use Cursor's session concept or conversation concept?
- How should we handle multi-conversation sessions?

### B. Time Window Queries
**Current State**:
- Traces have `timestamp` field
- Sessions have `started_at` / `ended_at`
- No day-level grouping exposed

**Questions**:
- How should we define "today"? (calendar day? last 24 hours?)
- How should we handle timezone?
- Should time windows be inclusive or exclusive?

### C. Workspace vs. Project Mapping
**Current State**:
- Traces have `workspace_hash`
- Sessions have `workspace_path`
- No project/repo mapping

**Questions**:
- How should we map workspaces to projects?
- Should we track git repo information?
- How should we handle multi-repo workspaces?

### D. Privacy & Data Access
**Current State**:
- All data is local (SQLite)
- No cloud transmission
- No user identification

**Questions**:
- How should we handle data retention?
- Should users be able to delete their data?
- How should we handle data export?

---

## Key Decisions Needed

1. **PR Tracking Approach**
   - [ ] Git hooks for PR detection?
   - [ ] Manual PR ID input (MVP)?
   - [ ] Commit timestamp-based detection?
   - [ ] Branch name tracking?

2. **API Design**
   - [ ] REST API vs. direct SQLite access?
   - [ ] Single endpoint vs. multiple endpoints?
   - [ ] Pagination strategy?
   - [ ] Response format (JSON schema)?

3. **Journaling Storage**
   - [ ] Separate table vs. linked to sessions?
   - [ ] Required vs. optional?
   - [ ] How to link to PRs?

4. **Day/Session Grouping**
   - [ ] Calendar day vs. rolling 24 hours?
   - [ ] Timezone handling?
   - [ ] Session boundaries?

5. **Model Metadata**
   - [ ] How to handle missing model info?
   - [ ] Model family grouping?
   - [ ] Cost calculation approach?

---

## Action Items (Post-Meeting)

### For Aaron (Analytics Layer)
- [ ] Implement day-level grouping logic
- [ ] Design and implement API layer (REST or query functions)
- [ ] Create SessionSummary, DailySummary data structures
- [ ] Design PR tracking approach (MVP)
- [ ] Enhance model metadata extraction
- [ ] Document data contracts (JSON schemas)

### For Ryan (Skills Layer)
- [ ] Finalize journaling questions (minimal set)
- [ ] Design skills that consume analytics API
- [ ] Test integration with analytics layer
- [ ] Refine prompt engineering for insights
- [ ] Design output formatting for insights

### Joint
- [ ] Define JSON schemas for data contracts
- [ ] Test end-to-end flow (traces → analytics → skills → insights)
- [ ] Iterate on API design based on skills needs
- [ ] Document integration patterns

---

## Questions to Ask Ryan

### Vision & Use Cases
1. "If Blueplane nailed introspection for you personally, what would it show?"
2. "What's the most valuable insight you'd want from a PR?"
3. "How do you want to consume these insights?" (CLI, dashboard, attached to PR?)

### Data Contracts
4. "If you could call one API for the long-form report skill, what would you want it to return?"
5. "What's the minimal data structure that would be useful?"
6. "Do these JSON schemas make sense for your skills?"

### PR Tracking
7. "How do you envision triggering PR insights?" (on PR open? on demand?)
8. "What's the minimal PR tracking we need to make this work?"
9. "Can we start with manual PR ID input, then automate later?"

### Journaling
10. "Which journaling questions would make the biggest difference for insights?"
11. "What's the minimal journaling set that still provides value?"
12. "How should we store journaling answers?" (separate table? linked to sessions?)

### Model Metadata
13. "What would you do differently if you knew the model per call?"
14. "What model-specific insights would be most valuable?"
15. "How should we handle traces without model info?"

### Next Steps
16. "Which experience would be most valuable to you right now?"
17. "What's the fastest path to a working demo?"
18. "Can we start with manual PR ID input for PR insights?"

---

## Appendix: Current Codebase References

### Key Files
- **Schema**: `src/processing/database/schema.py`
  - `cursor_raw_traces` table (lines 286-378)
  - `cursor_sessions` table (lines 23-56)
  - `conversations` table (lines 59+)
- **Analytics**: `src/analytics/` (early stage)
- **Model Extraction**: `feature/cursor-model-extraction` branch
- **Documentation**: 
  - `docs/analytics/ANALYTICS_USER_STORIES.md`
  - `docs/analytics/ANALYTICS_FEATURE_ROADMAP.md`
  - `docs/analytics/ANALYTICS_USER_PERSONAS.md`

### Current Data State
- **Cursor Traces**: 51,925 (sequence 1-51,925)
- **Claude Traces**: 0
- **Sessions**: 196
- **Conversations**: 142
- **Analytics Processing**: ~1% complete (540/51,925)

---

*Prepared by: Aaron (Analytics Layer)*  
*Date: December 3, 2025*  
*Based on: Sync meeting notes + codebase analysis*

